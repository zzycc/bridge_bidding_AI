{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "from sys import getsizeof\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the csv file for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "50000\n",
      "50000\n",
      "50000\n",
      "200000\n"
     ]
    }
   ],
   "source": [
    "######### Use pandas to read \"bridge.csv\",\"combats.csv\" file #########\n",
    "\n",
    "\n",
    "bridge_dfN = pd.read_csv('N.csv')\n",
    "bridge_dfS = pd.read_csv('S.csv')  \n",
    "bridge_dfE = pd.read_csv('E.csv')  \n",
    "bridge_dfW = pd.read_csv('W.csv')\n",
    "\n",
    "#bridge_dfN = pd.DataFrame(df_N)\n",
    "#bridge_dfS = pd.DataFrame(df_S)\n",
    "#bridge_dfE = pd.DataFrame(df_E)\n",
    "#bridge_dfW = pd.DataFrame(df_W)\n",
    "\n",
    "\n",
    "\n",
    "bridge_dfN = bridge_dfN.drop('lin_file',axis = 1)\n",
    "bridge_dfN = bridge_dfN.drop('board_num',axis = 1)\n",
    "bridge_dfS = bridge_dfS.drop('lin_file',axis = 1)\n",
    "bridge_dfS = bridge_dfS.drop('board_num',axis = 1)\n",
    "bridge_dfE = bridge_dfE.drop('lin_file',axis = 1)\n",
    "bridge_dfE = bridge_dfE.drop('board_num',axis = 1)\n",
    "bridge_dfW = bridge_dfW.drop('lin_file',axis = 1)\n",
    "bridge_dfW = bridge_dfW.drop('board_num',axis = 1)\n",
    "\n",
    "\n",
    "bridge_dfN['direction'] = 'N'\n",
    "bridge_dfS['direction'] = 'S'\n",
    "bridge_dfE['direction'] = 'E'\n",
    "bridge_dfW['direction'] = 'W'\n",
    "'''\n",
    "bridge_dfN_test = bridge_dfN[500000:505000]\n",
    "bridge_dfS_test = bridge_dfS[500000:505000]\n",
    "bridge_dfE_test = bridge_dfE[500000:505000]\n",
    "bridge_dfW_test = bridge_dfW[500000:505000]\n",
    "'''\n",
    "bridge_dfN = bridge_dfN[:50000]\n",
    "bridge_dfS = bridge_dfS[:50000]\n",
    "bridge_dfE = bridge_dfE[:50000]\n",
    "bridge_dfW = bridge_dfW[:50000]\n",
    "\n",
    "\n",
    "\n",
    "bridge_df = pd.concat([bridge_dfN, bridge_dfS], ignore_index=True)\n",
    "bridge_df = pd.concat([bridge_df, bridge_dfE], ignore_index=True)\n",
    "bridge_df = pd.concat([bridge_df, bridge_dfW], ignore_index=True)\n",
    "#bridge_df = bridge_df.astype('int')\n",
    "#df = pd.read_csv('combats.csv') \n",
    "#combats_df = df\n",
    "#print(combats_df.iloc[:,:])\n",
    "#print(len(combats_df.iloc[:,:]))\n",
    "print(len(bridge_dfN.iloc[:,:]))\n",
    "print(len(bridge_dfS.iloc[:,:]))\n",
    "print(len(bridge_dfE.iloc[:,:]))\n",
    "print(len(bridge_dfW.iloc[:,:]))\n",
    "\n",
    "print(len(bridge_df.iloc[:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       table_num value  self_value  op_value  \\\n",
      "49998          1  none           0         0   \n",
      "49999          2    NS           1         0   \n",
      "50000          1  none           0         0   \n",
      "50001          1  none           0         0   \n",
      "\n",
      "                            bidding_seq  order suit_num sort_suit_num  \\\n",
      "49998   p|p|p|1C!|p|1D!|p|1S|p|4S|p|p|p      1  1|5|4|3       5|4|3|1   \n",
      "49999                  1H|2D|p|3N|p|p|p      4  4|3|1|5       5|4|3|1   \n",
      "50000                        p|2S|p|p|p      3  2|4|3|4       4|4|3|2   \n",
      "50001  1C|1S|d|r|p|p|2C|2S|p|p|3C|p|p|p      3  2|4|3|4       4|4|3|2   \n",
      "\n",
      "                    suit             suits  HCP_dis  HCP    short     void  \\\n",
      "49998  SQHQJ765DT863CQ43  Q|QJ765|T863|Q43  2|3|0|2    7  1|0|0|0  0|0|0|0   \n",
      "49999  SQJ54HKJ8D4CKJ863  QJ54|KJ8|4|KJ863  3|4|0|4   11  0|0|1|0  0|0|0|0   \n",
      "50000  S84H8542DA93CAK94  84|8542|A93|AK94  0|0|4|7   11  0|0|0|0  0|0|0|0   \n",
      "50001  S84H8542DA93CAK94  84|8542|A93|AK94  0|0|4|7   11  0|0|0|0  0|0|0|0   \n",
      "\n",
      "          long  winner_dis  winner  num_of_A direction  \n",
      "49998  0|1|0|0  0|2.75|1|0    3.75         0         N  \n",
      "49999  0|0|0|1  1.75|1|0|3    5.75         0         N  \n",
      "50000  0|0|0|0     0|1|1|3    5.00         2         S  \n",
      "50001  0|0|0|0     0|1|1|3    5.00         2         S  \n"
     ]
    }
   ],
   "source": [
    "print(bridge_df.iloc[49998:50002])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.   0.   0.   0.   0.   9.   0.   0.   0.  -1.  -1.  -1.  -1.  -1.\n",
      " -1.  -1.   1.   3.   3.   4.   3.   0.   0.   0.   0.   0.   1.   1.\n",
      "  0.   0.   0.   0.   1.   0.   0.   1.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   1.   1.   0.   0.   0.   0.   1.   0.   1.   1.   0.\n",
      "  0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   1.   1.   0.   0.\n",
      "  0.   0.   1.   1.   7.   2.   2.  12.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   2.5  1.   0.   3.5  1.   0. ]\n",
      "[12.   1.   1.   0.   0.   5.  36.   8.   0.   0.   0.  -1.  -1.  -1.\n",
      " -1.  -1.   2.   4.   2.   4.   3.   0.   0.   1.   0.   1.   1.   0.\n",
      "  0.   0.   0.   0.   1.   0.   0.   1.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   1.   0.   0.   1.\n",
      "  1.   0.   0.   1.   0.   1.   0.   0.   0.   0.   0.   1.   0.   0.\n",
      "  0.   0.   1.   1.   6.   2.   6.  15.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   1.   1.5  1.   1.5  5.   2.   0. ]\n",
      "[[ 1.   0.   0.  ...  3.5  1.   0. ]\n",
      " [ 1.   0.   0.  ...  3.5  1.   0. ]\n",
      " [ 2.   1.   1.  ...  2.5  0.   0. ]\n",
      " ...\n",
      " [20.   3.   1.  ...  4.5  0.   3. ]\n",
      " [ 1.   0.   0.  ... 11.   3.   3. ]\n",
      " [ 1.   0.   0.  ... 11.   3.   3. ]]\n",
      "(199999, 97)\n"
     ]
    }
   ],
   "source": [
    "#print(bridge_dfW)  #set seq = 30num\n",
    "\n",
    "train_data=[]\n",
    "\n",
    "for i in range(len(bridge_df.iloc[:,:])-1):\n",
    "    if bridge_df.iloc[i,1] == 'none':\n",
    "        bridge_df.iloc[i,1] = 0\n",
    "    elif bridge_df.iloc[i,1] == 'NS':\n",
    "        bridge_df.iloc[i,1] = 1\n",
    "    elif bridge_df.iloc[i,1] == 'EW':\n",
    "        bridge_df.iloc[i,1] = 2\n",
    "    elif bridge_df.iloc[i,1] == 'both':\n",
    "        bridge_df.iloc[i,1] = 3\n",
    "    split_list = bridge_df.iloc[i,4].split('|')\n",
    "    temp = 0\n",
    "    temp_list_bidding =[-1]*12\n",
    "    for j in range(len(split_list)):\n",
    "        if(j>11):\n",
    "            break;\n",
    "        split_list[j] = split_list[j].replace(\"!\", \"\")\n",
    "        if(split_list[j][0]=='p' or split_list[j][0]=='P'):\n",
    "            temp_list_bidding[j]=0\n",
    "            continue\n",
    "        if(split_list[j][0]=='d' or split_list[j][0]=='D'):\n",
    "            temp_list_bidding[j]=36\n",
    "            continue\n",
    "        if(split_list[j][0]=='r' or split_list[j][0]=='R'):\n",
    "            temp_list_bidding[j]=37\n",
    "            continue\n",
    "                \n",
    "                \n",
    "        temp = 0\n",
    "        if(len(split_list[j]) != 2):\n",
    "            temp = -1\n",
    "            break\n",
    "        if(split_list[j][0].isdigit()):\n",
    "            temp += (int(split_list[j][0])-1)*5\n",
    "        if(split_list[j][1] == 'C' or split_list[j][1] == 'c'):\n",
    "            temp += 1\n",
    "        elif(split_list[j][1] == 'D' or split_list[j][1] == 'd' ):\n",
    "            temp += 2\n",
    "        elif(split_list[j][1] == 'H' or split_list[j][1] == 'h'):\n",
    "            temp += 3\n",
    "        elif(split_list[j][1] == 'S' or split_list[j][1] == 's'):\n",
    "            temp += 4\n",
    "        elif(split_list[j][1] == 'N' or split_list[j][1] == 'n'):\n",
    "            temp += 5\n",
    "        else:\n",
    "            temp = -1\n",
    "            break\n",
    "        temp_list_bidding[j] = temp\n",
    "        \n",
    "    temp_list = []\n",
    "    temp_list.append(bridge_df.iloc[i,0])\n",
    "    temp_list.append(bridge_df.iloc[i,1])\n",
    "    temp_list.append(bridge_df.iloc[i,2])\n",
    "    temp_list.append(bridge_df.iloc[i,3])\n",
    "    for j in range(12):\n",
    "        temp_list.append(temp_list_bidding[j])\n",
    "    temp_list.append(bridge_df.iloc[i,5])\n",
    "\n",
    "    split_list6 = bridge_df.iloc[i,6].split('|')\n",
    "    for j in range(4):\n",
    "        temp_list.append(split_list6[j])\n",
    "    '''\n",
    "    split_list7 = bridge_df.iloc[i,7].split('|')\n",
    "    for j in range(4):\n",
    "        temp_list.append(split_list7[j])\n",
    "    '''\n",
    "    split_list9 = bridge_df.iloc[i,9].split('|')    \n",
    "    for j in range(4):\n",
    "        suit = [0]*13\n",
    "        for k in range(len(split_list9[j])):\n",
    "            if split_list9[j][k] == 'A':\n",
    "                suit[1] = 1\n",
    "            elif split_list9[j][k] == 'K':\n",
    "                suit[0] = 1\n",
    "            elif split_list9[j][k] == 'Q':\n",
    "                suit[12] = 1\n",
    "            elif split_list9[j][k] == 'J':\n",
    "                suit[11] = 1\n",
    "            elif split_list9[j][k] == 'T':\n",
    "                suit[10] = 1\n",
    "            elif split_list9[j][k].isdigit():\n",
    "                suit[int(split_list9[j][k])] = 1\n",
    "            else:\n",
    "                suit[k] = -1\n",
    "        temp_list = temp_list + suit\n",
    "        \n",
    "    split_list10 = bridge_df.iloc[i,10].split('|')\n",
    "    for j in range(4):\n",
    "        temp_list.append(split_list10[j])\n",
    "    temp_list.append(bridge_df.iloc[i,11])\n",
    "    \n",
    "    split_list12 = bridge_df.iloc[i,12].split('|')\n",
    "    for j in range(4):\n",
    "        temp_list.append(split_list12[j])\n",
    "    split_list13 = bridge_df.iloc[i,13].split('|')\n",
    "    for j in range(4):\n",
    "        temp_list.append(split_list13[j])\n",
    "    split_list14 = bridge_df.iloc[i,14].split('|')\n",
    "    for j in range(4):\n",
    "        temp_list.append(split_list14[j])\n",
    "    split_list15 = bridge_df.iloc[i,15].split('|')\n",
    "    for j in range(4):\n",
    "        temp_list.append(split_list15[j])\n",
    "    \n",
    "    temp_list.append(bridge_df.iloc[i,16])\n",
    "    temp_list.append(bridge_df.iloc[i,17])\n",
    "    if(bridge_df.iloc[i,18] == 'N'):\n",
    "        temp_list.append(0)\n",
    "    elif(bridge_df.iloc[i,18] == 'S'):\n",
    "        temp_list.append(1)\n",
    "    elif(bridge_df.iloc[i,18] == 'E'):\n",
    "        temp_list.append(2)\n",
    "    elif(bridge_df.iloc[i,18] == 'W'):\n",
    "        temp_list.append(3)\n",
    "    else:\n",
    "        temp_list.append(-1)\n",
    "        \n",
    "    \n",
    "    train_data.append(temp_list)\n",
    "\n",
    "train_data = np.asarray(train_data)\n",
    "train_data = train_data.astype('float')\n",
    "print(train_data[0])\n",
    "print(train_data[999])\n",
    "print(train_data)\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0fc35954ebae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_data_org\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_data_backup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data_org = train_data\n",
    "train_data_backup = train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.   0.   0.   0.   0.   9.   0.   0.   0.  -1.  -1.  -1.  -1.  -1.\n",
      " -1.  -1.   1.   3.   3.   4.   3.   0.   0.   0.   0.   0.   1.   1.\n",
      "  0.   0.   0.   0.   1.   0.   0.   1.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   1.   1.   0.   0.   0.   0.   1.   0.   1.   1.   0.\n",
      "  0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   1.   1.   0.   0.\n",
      "  0.   0.   1.   1.   7.   2.   2.  12.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   2.5  1.   0.   3.5  1.   0. ]\n",
      "[12.   1.   1.   0.   0.   5.  36.   8.   0.   0.   0.  -1.  -1.  -1.\n",
      " -1.  -1.   2.   4.   2.   4.   3.   0.   0.   1.   0.   1.   1.   0.\n",
      "  0.   0.   0.   0.   1.   0.   0.   1.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   1.   0.   0.   1.\n",
      "  1.   0.   0.   1.   0.   1.   0.   0.   0.   0.   0.   1.   0.   0.\n",
      "  0.   0.   1.   1.   6.   2.   6.  15.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   1.   1.5  1.   1.5  5.   2.   0. ]\n",
      "[[ 1.   0.   0.  ...  3.5  1.   0. ]\n",
      " [ 1.   0.   0.  ...  3.5  1.   0. ]\n",
      " [ 2.   1.   1.  ...  2.5  0.   0. ]\n",
      " ...\n",
      " [20.   3.   1.  ...  4.5  0.   3. ]\n",
      " [ 1.   0.   0.  ... 11.   3.   3. ]\n",
      " [ 1.   0.   0.  ... 11.   3.   3. ]]\n",
      "(199999, 97)\n"
     ]
    }
   ],
   "source": [
    "#np.savetxt('train_data_savefile.txt', train_data, fmt='%f')\n",
    "train_data = []\n",
    "train_data = np.loadtxt('train_data_withFeaure_savefile.txt', dtype=float)\n",
    "print(train_data[0])\n",
    "print(train_data[999])\n",
    "print(train_data)\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02        0.          0.          0.          0.          0.24324324\n",
      "  0.          0.          0.         -0.02702703 -0.02702703 -0.02702703\n",
      " -0.02702703 -0.02702703 -0.02702703 -1.          0.25        0.23076923\n",
      "  0.23076923  0.30769231  3.          0.          0.          0.\n",
      "  0.          0.          1.          1.          0.          0.\n",
      "  0.          0.          1.          0.          0.          1.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          1.          1.          0.\n",
      "  0.          0.          0.          1.          0.          1.\n",
      "  1.          0.          0.          0.          0.          1.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  1.          1.          0.          0.          0.          0.\n",
      "  1.          0.07692308  0.53846154  0.15384615  2.         12.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.25        0.1         0.          3.5         0.1\n",
      "  0.        ]\n",
      "[[ 0.02        0.          0.         ...  3.5         0.1\n",
      "   0.        ]\n",
      " [ 0.02        0.          0.         ...  3.5         0.1\n",
      "   0.        ]\n",
      " [ 0.04        0.33333333  1.         ...  2.5         0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.4         1.          1.         ...  4.5         0.\n",
      "   0.3       ]\n",
      " [ 0.02        0.          0.         ... 11.          0.3\n",
      "   0.3       ]\n",
      " [ 0.02        0.          0.         ... 11.          0.3\n",
      "   0.3       ]]\n",
      "(199999, 97)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_data[:,0] = train_data[:,0]/50\n",
    "train_data[:,1] = train_data[:,1]/3\n",
    "train_data[:,4:15] = train_data[:,4:15]/37\n",
    "train_data[:,16] = train_data[:,16]/4\n",
    "train_data[:,17:20] = train_data[:,17:20]/13\n",
    "train_data[:,73:76] = train_data[:,73:76]/13\n",
    "train_data[:,81] = train_data[:,81]/30\n",
    "train_data[:,91:94] = train_data[:,91:94]/10\n",
    "train_data[:,95] = train_data[:,95]/10\n",
    "train_data[:,96] = train_data[:,96]/10\n",
    "'''\n",
    "train_data[:,0] = train_data[:,0]*50\n",
    "train_data[:,1] = train_data[:,1]*3\n",
    "train_data[:,4] = train_data[:,4]*2000\n",
    "train_data[:,5] = train_data[:,5]*4\n",
    "train_data[:,6:13] = train_data[:,6:13]*13\n",
    "train_data[:,66:69] = train_data[:,66:69]*13\n",
    "train_data[:,70] = train_data[:,70]*30\n",
    "train_data[:,83:86] = train_data[:,83:86]*10\n",
    "train_data[:,87] = train_data[:,87]*10\n",
    "'''\n",
    "print(train_data[0,:])\n",
    "print(train_data)\n",
    "print(train_data.shape)\n",
    "print(type(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make one-hot format labels (But bridge seems to use 13-hot-encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0]\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n",
      "(199999, 38)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "#train_labels[i][1]:\n",
    "#combats_df.iloc[:,2]\n",
    "\n",
    "combats_train_hot_label = [[0 for x in range(38)] for y in range(199999)]\n",
    "\n",
    "for i in range(len(bridge_df.iloc[:,:])-1):\n",
    "    split_list = bridge_df.iloc[i,4].split('|')\n",
    "    order = int(bridge_df.iloc[i,5])-1\n",
    "    \n",
    "    if(order+1>len(split_list)):\n",
    "        combats_train_hot_label[i][0] = 1\n",
    "        continue\n",
    "    \n",
    "    split_list[order] = split_list[order].replace(\"!\", \"\")\n",
    "    if(split_list[order]=='p' or split_list[order]=='p!' or split_list[order]=='P' or split_list[order]=='P!'):\n",
    "        combats_train_hot_label[i][0] = 1\n",
    "        \n",
    "    elif(split_list[order]=='d' or split_list[order]=='d!' or split_list[order]=='D' or split_list[order]=='D!'):\n",
    "        combats_train_hot_label[i][36] = 1\n",
    "    elif(split_list[order]=='r' or split_list[order]=='r!' or split_list[order]=='R' or split_list[order]=='R!'):\n",
    "        combats_train_hot_label[i][37] = 1\n",
    "    temp_num = 0    \n",
    "    if (len(split_list[order]) == 2 and split_list[order][0].isdigit()):\n",
    "        temp_num = (int(split_list[order][0])-1)*5\n",
    "        if(split_list[order][1] == 'C' or split_list[order][1] == 'c'):\n",
    "            temp_num += 1\n",
    "        elif(split_list[order][1] == 'D' or split_list[order][1] == 'd' ):\n",
    "            temp_num += 2\n",
    "        elif(split_list[order][1] == 'H' or split_list[order][1] == 'h'):\n",
    "            temp_num += 3\n",
    "        elif(split_list[order][1] == 'S' or split_list[order][1] == 's'):\n",
    "            temp_num += 4\n",
    "        elif(split_list[order][1] == 'N' or split_list[order][1] == 'n'): \n",
    "            temp_num += 5\n",
    "        else:\n",
    "            temp_num = 0\n",
    "        combats_train_hot_label[i][temp_num] = 1 \n",
    "    \n",
    "\n",
    "#combats_train_hot_label = np_utils.to_categorical(combats_train_label,2)\n",
    "\n",
    "combats_train_hot_label = np.asarray(combats_train_hot_label)\n",
    "print(combats_train_hot_label[56][:])\n",
    "print(combats_train_hot_label)\n",
    "print(combats_train_hot_label.shape)\n",
    "print(type(combats_train_hot_label))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n",
      "(199999, 38)\n"
     ]
    }
   ],
   "source": [
    "np.savetxt('combats_train_hot_label_save_file.txt', combats_train_hot_label, fmt='%f')\n",
    "combats_train_hot_label = np.loadtxt('combats_train_hot_label_withFeaure_save_file.txt', dtype=float)\n",
    "print(combats_train_hot_label[0])\n",
    "print(combats_train_hot_label[999])\n",
    "print(combats_train_hot_label)\n",
    "print(combats_train_hot_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199999, 38)\n",
      "<class 'numpy.ndarray'>\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n",
      "(199999, 97)\n",
      "<class 'numpy.ndarray'>\n",
      "[ 0.02        0.          0.          0.          0.          0.24324324\n",
      "  0.          0.          0.         -0.02702703 -0.02702703 -0.02702703\n",
      " -0.02702703 -0.02702703 -0.02702703 -1.          0.25        0.23076923\n",
      "  0.23076923  0.30769231  3.          0.          0.          0.\n",
      "  0.          0.          1.          1.          0.          0.\n",
      "  0.          0.          1.          0.          0.          1.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          1.          1.          0.\n",
      "  0.          0.          0.          1.          0.          1.\n",
      "  1.          0.          0.          0.          0.          1.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  1.          1.          0.          0.          0.          0.\n",
      "  1.          0.07692308  0.53846154  0.15384615  2.         12.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.25        0.1         0.          3.5         0.1\n",
      "  0.        ]\n",
      "[[ 0.02        0.          0.         ...  3.5         0.1\n",
      "   0.        ]\n",
      " [ 0.02        0.          0.         ...  3.5         0.1\n",
      "   0.        ]\n",
      " [ 0.04        0.33333333  1.         ...  2.5         0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.4         1.          1.         ...  4.5         0.\n",
      "   0.3       ]\n",
      " [ 0.02        0.          0.         ... 11.          0.3\n",
      "   0.3       ]\n",
      " [ 0.02        0.          0.         ... 11.          0.3\n",
      "   0.3       ]]\n"
     ]
    }
   ],
   "source": [
    "#combats_train_hot_label = to_categorical(combats_train_hot_label,2)\n",
    "combats_train_hot_label = combats_train_hot_label.astype(float)\n",
    "print(combats_train_hot_label.shape)\n",
    "print(type(combats_train_hot_label))\n",
    "print(combats_train_hot_label[0,:])\n",
    "print(combats_train_hot_label)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(type(train_data))\n",
    "print(train_data[0,:])\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Create network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Activation\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "import keras.callbacks\n",
    "import os\n",
    "import os.path\n",
    "import glob\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Embedding\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    ###   code here  #####\n",
    "    #keras.layers.normalization.BatchNormalization(epsilon=1e-06, mode=0, momentum=0.9, weights=None)\n",
    "    \n",
    "    model.add(Dense(128,input_shape = (97,))) #need to check the input size, first set the input node to be 128 as matlab do\n",
    "    model.add(Activation('relu')) #might be leakyRelu, since they used the parameter alpha, still need to check\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.7))\n",
    "    \n",
    "    model.add(Dense(784))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.7))\n",
    "    \n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.7))\n",
    "\n",
    "    model.add(Dense(38,activation='softmax')) #need to check the output size, seems to be 36\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               12544     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 784)               101136    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 38)                4902      \n",
      "=================================================================\n",
      "Total params: 223,222\n",
      "Trainable params: 221,142\n",
      "Non-trainable params: 2,080\n",
      "_________________________________________________________________\n",
      "Train on 179999 samples, validate on 20000 samples\n",
      "Epoch 1/1000\n",
      "179999/179999 [==============================] - 3s 18us/step - loss: 3.3610 - acc: 0.3394 - val_loss: 2.5071 - val_acc: 0.4969\n",
      "Epoch 2/1000\n",
      "179999/179999 [==============================] - 1s 6us/step - loss: 2.5039 - acc: 0.4782 - val_loss: 2.1196 - val_acc: 0.5431\n",
      "Epoch 3/1000\n",
      "179999/179999 [==============================] - 1s 6us/step - loss: 2.2387 - acc: 0.5028 - val_loss: 1.9096 - val_acc: 0.5470\n",
      "Epoch 4/1000\n",
      "179999/179999 [==============================] - 1s 6us/step - loss: 2.0766 - acc: 0.5181 - val_loss: 1.8162 - val_acc: 0.5386\n",
      "Epoch 5/1000\n",
      "179999/179999 [==============================] - 1s 6us/step - loss: 1.9592 - acc: 0.5270 - val_loss: 1.7382 - val_acc: 0.5365\n",
      "Epoch 6/1000\n",
      "179999/179999 [==============================] - 1s 5us/step - loss: 1.8644 - acc: 0.5359 - val_loss: 1.6796 - val_acc: 0.5397\n"
     ]
    }
   ],
   "source": [
    "############## Start Training ############\n",
    "# use validation_data=(valid_data,valid_labels) in model.fit !!!!!! #\n",
    "'''\n",
    "training parameters\n",
    "update_dnntype = 2; \n",
    "badupdate_dnn = 2; \n",
    "explore_first = 1; \n",
    "alphaupdate_dnn = 0.1;\n",
    "batchsizeupdate_dnn = 50;\n",
    "batchsize = 50;\n",
    "decayRate = 0.98;\n",
    "momentum = 0.82;\n",
    "alpha = 0.83;\n",
    "startbackprop = 0;\n",
    "input = 52+36+5;\n",
    "lsize = 128;\n",
    "layer = 4;\n",
    "output = 36;\n",
    "eta = 0.05;\n",
    "'''\n",
    "earlyStopping = keras.callbacks.EarlyStopping(monitor='val_acc', patience=20, verbose=1, mode='auto')\n",
    "model = build_model()\n",
    "sgd = optimizers.SGD(lr=0.2, clipnorm=1.0\n",
    "                     ,decay = 1e-08 #original 1e-08\n",
    "                    )\n",
    "adadelta = optimizers.Adadelta(lr=1.0, epsilon=None, decay=0.0)\n",
    "model.compile(optimizer= adadelta#as bridge\n",
    "            #,loss='categorical_crossentropy'\n",
    "            ,loss='categorical_crossentropy'\n",
    "            ,metrics=['accuracy'])\n",
    "\n",
    "record = model.fit(train_data,combats_train_hot_label\n",
    "                   ,batch_size=4096 #original 64\n",
    "                   ,epochs=1000\n",
    "                   ,validation_split = 0.1\n",
    "                   ,callbacks=[earlyStopping]\n",
    "                   ,shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('bridge_model_withFeaure.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(train_data, combats_train_hot_label, verbose=0)\n",
    "print('Train loss:', score[0])\n",
    "print('Train accuracy:', score[1])\n",
    "plt.plot(record.history['acc'],label='acc')\n",
    "plt.plot(record.history['val_acc'],label='val_acc')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "plt.plot(record.history['loss'],label='loss')\n",
    "plt.plot(record.history['val_loss'],label='val_loss')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import csv\n",
    "bridge_df_test = []\n",
    "\n",
    "bridge_df_test = pd.concat([bridge_dfN_test, bridge_dfS_test], ignore_index=True)\n",
    "bridge_df_test = pd.concat([bridge_df_test, bridge_dfE_test], ignore_index=True)\n",
    "bridge_df_test = pd.concat([bridge_df_test, bridge_dfW_test], ignore_index=True)\n",
    "\n",
    "bridge_df_test.to_csv(\"bridge_test\", encoding='utf-8', index=False)'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('bridge_test.csv') \n",
    "test_df = df\n",
    "\n",
    "\n",
    "print(len(test_df.iloc[:,:]))\n",
    "print(test_df.iloc[:,:].shape)\n",
    "#print(test_df)\n",
    "\n",
    "test_data=[]\n",
    "\n",
    "for i in range(len(test_df.iloc[:,:])-1):\n",
    "    if test_df.iloc[i,1] == 'none':\n",
    "        test_df.iloc[i,1] = 0\n",
    "    elif test_df.iloc[i,1] == 'NS':\n",
    "        test_df.iloc[i,1] = 1\n",
    "    elif test_df.iloc[i,1] == 'EW':\n",
    "        test_df.iloc[i,1] = 2\n",
    "    elif test_df.iloc[i,1] == 'both':\n",
    "        test_df.iloc[i,1] = 3\n",
    "    split_list = test_df.iloc[i,4].split('|')\n",
    "    temp = 0\n",
    "    temp_list_bidding =[-1]*12\n",
    "    for j in range(len(split_list)):\n",
    "        if(j>11):\n",
    "            break;\n",
    "        split_list[j] = split_list[j].replace(\"!\", \"\")\n",
    "        if(split_list[j][0]=='p' or split_list[j][0]=='P'):\n",
    "            temp_list_bidding[j]=0\n",
    "            continue\n",
    "        if(split_list[j][0]=='d' or split_list[j][0]=='D'):\n",
    "            temp_list_bidding[j]=36\n",
    "            continue\n",
    "        if(split_list[j][0]=='r' or split_list[j][0]=='R'):\n",
    "            temp_list_bidding[j]=37\n",
    "            continue\n",
    "                \n",
    "                \n",
    "        temp = 0\n",
    "        if(len(split_list[j]) != 2):\n",
    "            temp = -1\n",
    "            break\n",
    "        if(split_list[j][0].isdigit()):\n",
    "            temp += (int(split_list[j][0])-1)*5\n",
    "        if(split_list[j][1] == 'C' or split_list[j][1] == 'c'):\n",
    "            temp += 1\n",
    "        elif(split_list[j][1] == 'D' or split_list[j][1] == 'd' ):\n",
    "            temp += 2\n",
    "        elif(split_list[j][1] == 'H' or split_list[j][1] == 'h'):\n",
    "            temp += 3\n",
    "        elif(split_list[j][1] == 'S' or split_list[j][1] == 's'):\n",
    "            temp += 4\n",
    "        elif(split_list[j][1] == 'N' or split_list[j][1] == 'n'):\n",
    "            temp += 5\n",
    "        else:\n",
    "            temp = -1\n",
    "            break\n",
    "        temp_list_bidding[j] = temp\n",
    "        \n",
    "    temp_list = []\n",
    "    temp_list.append(test_df.iloc[i,0])\n",
    "    temp_list.append(test_df.iloc[i,1])\n",
    "    temp_list.append(test_df.iloc[i,2])\n",
    "    temp_list.append(test_df.iloc[i,3])\n",
    "    for j in range(12):\n",
    "        temp_list.append(temp_list_bidding[j])\n",
    "    temp_list.append(test_df.iloc[i,5])\n",
    "\n",
    "    split_list6 = test_df.iloc[i,6].split('|')\n",
    "    for j in range(4):\n",
    "        temp_list.append(split_list6[j])\n",
    "    '''\n",
    "    split_list7 = test_df.iloc[i,7].split('|')\n",
    "    for j in range(4):\n",
    "        temp_list.append(split_list7[j])\n",
    "    '''\n",
    "    split_list9 = test_df.iloc[i,9].split('|')    \n",
    "    for j in range(4):\n",
    "        suit = [0]*13\n",
    "        for k in range(len(split_list9[j])):\n",
    "            if split_list9[j][k] == 'A':\n",
    "                suit[1] = 1\n",
    "            elif split_list9[j][k] == 'K':\n",
    "                suit[0] = 1\n",
    "            elif split_list9[j][k] == 'Q':\n",
    "                suit[12] = 1\n",
    "            elif split_list9[j][k] == 'J':\n",
    "                suit[11] = 1\n",
    "            elif split_list9[j][k] == 'T':\n",
    "                suit[10] = 1\n",
    "            elif split_list9[j][k].isdigit():\n",
    "                suit[int(split_list9[j][k])] = 1\n",
    "            else:\n",
    "                suit[k] = -1\n",
    "        temp_list = temp_list + suit\n",
    "        \n",
    "    split_list10 = test_df.iloc[i,10].split('|')\n",
    "    for j in range(4):\n",
    "        temp_list.append(split_list10[j])\n",
    "    temp_list.append(test_df.iloc[i,11])\n",
    "    \n",
    "    split_list12 = test_df.iloc[i,12].split('|')\n",
    "    for j in range(4):\n",
    "        temp_list.append(split_list12[j])\n",
    "    split_list13 = test_df.iloc[i,13].split('|')\n",
    "    for j in range(4):\n",
    "        temp_list.append(split_list13[j])\n",
    "    split_list14 = test_df.iloc[i,14].split('|')\n",
    "    for j in range(4):\n",
    "        temp_list.append(split_list14[j])\n",
    "    split_list15 = test_df.iloc[i,15].split('|')\n",
    "    for j in range(4):\n",
    "        temp_list.append(split_list15[j])\n",
    "    \n",
    "    temp_list.append(test_df.iloc[i,16])\n",
    "    temp_list.append(test_df.iloc[i,17])\n",
    "    if(test_df.iloc[i,18] == 'N'):\n",
    "        temp_list.append(0)\n",
    "    elif(test_df.iloc[i,18] == 'S'):\n",
    "        temp_list.append(1)\n",
    "    elif(test_df.iloc[i,18] == 'E'):\n",
    "        temp_list.append(2)\n",
    "    elif(test_df.iloc[i,18] == 'W'):\n",
    "        temp_list.append(3)\n",
    "    else:\n",
    "        temp_list.append(-1)\n",
    "        \n",
    "    \n",
    "    test_data.append(temp_list)\n",
    "\n",
    "test_data = np.asarray(test_data)\n",
    "test_data = test_data.astype('float')\n",
    "print(test_data[0])\n",
    "print(test_data[999])\n",
    "print(test_data)\n",
    "print(test_data.shape)\n",
    "\n",
    "test_data[:,0] = test_data[:,0]/50\n",
    "test_data[:,1] = test_data[:,1]/3\n",
    "test_data[:,4:15] = test_data[:,4:15]/37\n",
    "test_data[:,16] = test_data[:,16]/4\n",
    "test_data[:,17:20] = test_data[:,17:20]/13\n",
    "test_data[:,73:76] = test_data[:,73:76]/13\n",
    "test_data[:,81] = test_data[:,81]/30\n",
    "test_data[:,91:94] = test_data[:,91:94]/10\n",
    "test_data[:,95] = test_data[:,95]/10\n",
    "test_data[:,96] = test_data[:,96]/10\n",
    "\n",
    "\n",
    "print(test_data.shape)\n",
    "print(type(test_data))\n",
    "print(test_data[0,:])\n",
    "print(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### use \"model.predict(testing_data)\" to get the results ######\n",
    "pred = np.argmax(model.predict(test_data), axis=1).reshape(19999,1)\n",
    "df = pd.DataFrame(pred, columns = [\"card output\"])\n",
    "\n",
    "print(df)\n",
    "'''for i in range (19999):\n",
    "    if  df.at[i, 'winner'] == 0:\n",
    "        temp = test_df.iloc[i,0]\n",
    "        df.at[i, 'winner'] = temp\n",
    "    else:\n",
    "        temp = test_df.iloc[i,1]\n",
    "        df.at[i,'winner'] = temp'''\n",
    "        \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the result to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('result.csv',index_label = \"index\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 317,
   "position": {
    "height": "229px",
    "left": "1293px",
    "right": "20px",
    "top": "156px",
    "width": "623px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
